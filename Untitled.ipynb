{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "adfec82e-8211-44b4-97fa-64877aea6562",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data loaded successfully\n",
      "        City        Date  PM2_5  PM10     NO    NO2    NOx  NH3     CO    SO2  \\\n",
      "0  Ahmedabad  2015-01-01    NaN   NaN   0.92  18.22  17.15  NaN   0.92  27.64   \n",
      "1  Ahmedabad  2015-01-02    NaN   NaN   0.97  15.69  16.46  NaN   0.97  24.55   \n",
      "2  Ahmedabad  2015-01-03    NaN   NaN  17.40  19.30  29.70  NaN  17.40  29.07   \n",
      "3  Ahmedabad  2015-01-04    NaN   NaN   1.70  18.48  17.97  NaN   1.70  18.59   \n",
      "4  Ahmedabad  2015-01-05    NaN   NaN  22.10  21.42  37.76  NaN  22.10  39.33   \n",
      "\n",
      "       O3  Benzene  Toluene  Xylene  AQI AQI_Bucket  \n",
      "0  133.36     0.00     0.02    0.00  NaN        NaN  \n",
      "1   34.06     3.68     5.50    3.77  NaN        NaN  \n",
      "2   30.70     6.80    16.40    2.25  NaN        NaN  \n",
      "3   36.08     4.43    10.14    1.00  NaN        NaN  \n",
      "4   39.31     7.01    18.89    2.78  NaN        NaN  \n"
     ]
    }
   ],
   "source": [
    "# Import libraries\n",
    "import pandas as pd\n",
    "\n",
    "# Load the dataset\n",
    "data = pd.read_csv(\"city_day.csv\")\n",
    "\n",
    "# Rename columns for easier handling\n",
    "data.columns = [\n",
    "    \"City\", \"Date\", \"PM2_5\", \"PM10\", \"NO\", \"NO2\", \"NOx\", \"NH3\", \"CO\",\n",
    "    \"SO2\", \"O3\", \"Benzene\", \"Toluene\", \"Xylene\", \"AQI\", \"AQI_Bucket\"\n",
    "]\n",
    "\n",
    "# Preview data\n",
    "print(\"Data loaded successfully\")\n",
    "print(data.head())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "4fb8b517-0ea9-4cbc-82e3-a615759d503e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Missing values before handling:\n",
      " City              0\n",
      "Date              0\n",
      "PM2_5          4598\n",
      "PM10          11140\n",
      "NO             3582\n",
      "NO2            3585\n",
      "NOx            4185\n",
      "NH3           10328\n",
      "CO             2059\n",
      "SO2            3854\n",
      "O3             4022\n",
      "Benzene        5623\n",
      "Toluene        8041\n",
      "Xylene        18109\n",
      "AQI            4681\n",
      "AQI_Bucket     4681\n",
      "dtype: int64\n",
      "Missing values after handling:\n",
      " City          0\n",
      "Date          0\n",
      "PM2_5         0\n",
      "PM10          0\n",
      "NO            0\n",
      "NO2           0\n",
      "NOx           0\n",
      "NH3           0\n",
      "CO            0\n",
      "SO2           0\n",
      "O3            0\n",
      "Benzene       0\n",
      "Toluene       0\n",
      "Xylene        0\n",
      "AQI           0\n",
      "AQI_Bucket    0\n",
      "dtype: int64\n"
     ]
    }
   ],
   "source": [
    "# Count missing values in each column\n",
    "missing_values = data.isnull().sum()\n",
    "print(\"Missing values before handling:\\n\", missing_values)\n",
    "\n",
    "# List of numeric pollutant columns\n",
    "numeric_cols = [\n",
    "    \"PM2_5\", \"PM10\", \"NO\", \"NO2\", \"NOx\", \"NH3\", \"CO\",\n",
    "    \"SO2\", \"O3\", \"Benzene\", \"Toluene\", \"Xylene\", \"AQI\"\n",
    "]\n",
    "\n",
    "# Fill numeric missing values with median\n",
    "data[numeric_cols] = data[numeric_cols].fillna(data[numeric_cols].median())\n",
    "\n",
    "# Fill categorical missing values in AQI_Bucket with mode\n",
    "data['AQI_Bucket'] = data['AQI_Bucket'].fillna(data['AQI_Bucket'].mode()[0])\n",
    "\n",
    "# Verify that there are no missing values now\n",
    "print(\"Missing values after handling:\\n\", data.isnull().sum())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "0268528c-e482-4db1-8c7a-77aa05206e41",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Categorical variables encoded and label encoders saved.\n"
     ]
    }
   ],
   "source": [
    "from sklearn.preprocessing import LabelEncoder\n",
    "import joblib\n",
    "\n",
    "# Initialize label encoders\n",
    "le_city = LabelEncoder()\n",
    "le_aqi_bucket = LabelEncoder()\n",
    "\n",
    "# Fit and transform City and AQI_Bucket columns\n",
    "data['City'] = le_city.fit_transform(data['City'])\n",
    "data['AQI_Bucket'] = le_aqi_bucket.fit_transform(data['AQI_Bucket'])\n",
    "\n",
    "# Save the label encoders for deployment\n",
    "joblib.dump(le_city, \"le_city.pkl\")\n",
    "joblib.dump(le_aqi_bucket, \"le_aqi_bucket.pkl\")\n",
    "\n",
    "print(\"Categorical variables encoded and label encoders saved.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "566fe30d-42f7-483e-b694-57fb214f7333",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Feature engineering completed.\n"
     ]
    }
   ],
   "source": [
    "# Convert 'Date' to datetime datatype\n",
    "data['Date'] = pd.to_datetime(data['Date'])\n",
    "\n",
    "# Create pollutant ratio features to capture relative levels\n",
    "data['PM_ratio'] = data['PM2_5'] / (data['PM10'] + 1e-6)  # avoid division by zero\n",
    "data['NO_ratio'] = data['NOx'] / (data['NO2'] + 1e-6)\n",
    "\n",
    "# Extract day of week from the 'Date' column (0=Monday, ..., 6=Sunday)\n",
    "data['Day_of_week'] = data['Date'].dt.dayofweek\n",
    "\n",
    "# Calculate 3-day rolling averages for selected pollutants\n",
    "pollutants = [\n",
    "    'PM2_5', 'PM10', 'NO', 'NO2', 'NOx', 'NH3',\n",
    "    'CO', 'SO2', 'O3', 'Benzene', 'Toluene', 'Xylene'\n",
    "]\n",
    "\n",
    "for pollutant in pollutants:\n",
    "    data[f'{pollutant}_3d_avg'] = data.groupby('City')[pollutant].transform(\n",
    "        lambda x: x.rolling(window=3, min_periods=1).mean()\n",
    "    )\n",
    "\n",
    "print(\"Feature engineering completed.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "735dc95c-fe2e-409d-bab0-1f4538c0ad0b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Feature preparation, scaling, and train-test split complete.\n",
      "Train shape: (19195, 28), Test shape: (10336, 28)\n"
     ]
    }
   ],
   "source": [
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.model_selection import train_test_split\n",
    "import joblib\n",
    "\n",
    "# Exclude columns not used as features\n",
    "X = data.drop(columns=['Date', 'AQI_Bucket', 'AQI'])\n",
    "y = data['AQI_Bucket']\n",
    "\n",
    "# Identify numeric columns for scaling\n",
    "numeric_cols = X.select_dtypes(include=['float64', 'int64']).columns\n",
    "\n",
    "# Initialize and fit scaler\n",
    "scaler = StandardScaler()\n",
    "X[numeric_cols] = scaler.fit_transform(X[numeric_cols])\n",
    "\n",
    "# Save scaler and numeric columns for deployment\n",
    "joblib.dump(scaler, \"feature_scaler.pkl\")\n",
    "joblib.dump(list(numeric_cols), \"numeric_columns.pkl\")\n",
    "\n",
    "# Split into train and test sets (use stratify to maintain class distribution)\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X, y, test_size=0.35, random_state=42, stratify=y\n",
    ")\n",
    "\n",
    "print(\"Feature preparation, scaling, and train-test split complete.\")\n",
    "print(f\"Train shape: {X_train.shape}, Test shape: {X_test.shape}\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "b43cbd75-3ed6-4063-a435-68cc837d9737",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages/xgboost/training.py:199: UserWarning: [23:28:58] WARNING: /Users/runner/work/xgboost/xgboost/src/learner.cc:790: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000706 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 6664\n",
      "[LightGBM] [Info] Number of data points in the train set: 19195, number of used features: 28\n",
      "[LightGBM] [Info] Start training from score -3.091616\n",
      "[LightGBM] [Info] Start training from score -0.782060\n",
      "[LightGBM] [Info] Start training from score -2.362429\n",
      "[LightGBM] [Info] Start training from score -1.278488\n",
      "[LightGBM] [Info] Start training from score -3.093912\n",
      "[LightGBM] [Info] Start training from score -2.536598\n",
      "Voting ensemble model trained successfully.\n"
     ]
    }
   ],
   "source": [
    "# Import models\n",
    "from sklearn.ensemble import RandomForestClassifier, VotingClassifier\n",
    "from xgboost import XGBClassifier\n",
    "from lightgbm import LGBMClassifier\n",
    "from catboost import CatBoostClassifier\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "\n",
    "# Initialize individual classifiers\n",
    "rf_model = RandomForestClassifier(n_estimators=200, random_state=42)\n",
    "xgb_model = XGBClassifier(use_label_encoder=False, eval_metric='mlogloss', random_state=42)\n",
    "lgb_model = LGBMClassifier(random_state=42)\n",
    "cat_model = CatBoostClassifier(verbose=0, random_state=42)\n",
    "mlp_model = MLPClassifier(hidden_layer_sizes=(64, 32), max_iter=400, random_state=42)\n",
    "\n",
    "# Combine models into a Voting Classifier (soft voting)\n",
    "voting_model = VotingClassifier(\n",
    "    estimators=[\n",
    "        ('rf', rf_model),\n",
    "        ('xgb', xgb_model),\n",
    "        ('lgb', lgb_model),\n",
    "        ('cat', cat_model),\n",
    "        ('mlp', mlp_model)\n",
    "    ],\n",
    "    voting='soft'\n",
    ")\n",
    "\n",
    "# Train the ensemble on training data\n",
    "voting_model.fit(X_train, y_train)\n",
    "\n",
    "print(\"Voting ensemble model trained successfully.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "0c149de8-c6ad-4a0c-a201-a1976e0ddcec",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy on test set: 0.8437\n",
      "Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.82      0.70      0.75       469\n",
      "           1       0.89      0.88      0.89      4729\n",
      "           2       0.75      0.68      0.71       973\n",
      "           3       0.83      0.87      0.85      2879\n",
      "           4       0.83      0.81      0.82       468\n",
      "           5       0.76      0.82      0.79       818\n",
      "\n",
      "    accuracy                           0.84     10336\n",
      "   macro avg       0.81      0.79      0.80     10336\n",
      "weighted avg       0.84      0.84      0.84     10336\n",
      "\n",
      "Confusion Matrix:\n",
      "[[ 328   13    0  128    0    0]\n",
      " [  10 4162  130  392   14   21]\n",
      " [   0  196  658    1    3  115]\n",
      " [  63  292    6 2517    0    1]\n",
      " [   0    5    5    0  381   77]\n",
      " [   0    7   78    0   59  674]]\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import accuracy_score, classification_report, confusion_matrix\n",
    "\n",
    "# Predict on test data\n",
    "y_pred = voting_model.predict(X_test)\n",
    "\n",
    "# Calculate accuracy\n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "print(f\"Accuracy on test set: {accuracy:.4f}\")\n",
    "\n",
    "# Print classification report\n",
    "print(\"Classification Report:\")\n",
    "print(classification_report(y_test, y_pred))\n",
    "\n",
    "# Print confusion matrix\n",
    "print(\"Confusion Matrix:\")\n",
    "print(confusion_matrix(y_test, y_pred))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "23c86ffa-d5e1-473c-a7b4-5a13e265c9ea",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model, scaler, numeric columns, and label encoders saved successfully.\n"
     ]
    }
   ],
   "source": [
    "import joblib\n",
    "\n",
    "# Save the trained model\n",
    "joblib.dump(voting_model, \"stacking_ensemble.pkl\")\n",
    "\n",
    "# Save the scaler used for feature scaling\n",
    "joblib.dump(scaler, \"feature_scaler.pkl\")\n",
    "\n",
    "# Save the list of numeric feature columns\n",
    "joblib.dump(list(numeric_cols), \"numeric_columns.pkl\")\n",
    "\n",
    "# Save label encoders for City and AQI_Bucket\n",
    "joblib.dump(le_city, \"le_city.pkl\")\n",
    "joblib.dump(le_aqi_bucket, \"le_aqi_bucket.pkl\")\n",
    "\n",
    "print(\"Model, scaler, numeric columns, and label encoders saved successfully.\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
